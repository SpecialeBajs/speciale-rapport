\section{Related work}

\subsection{Light GCN}
In the paper \textit{LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation} Xiangnan et al. investigate the effect of feature transformation and nonlinear activation within collaborative filtering using Neural Graph Collaborative Filtering (NGCF) \cite{lightgcn}.
NGCF is a framework developed by Wang et al. \cite{NGCF_2019} that utilizes Graph Neural Network with three components in the framework: (1) an embedding layer that constructs initial user - and item embeddings; (2) embedding propagation layers that captures CF with the two operations of message construction and message aggregation; (3) a prediction layer that concatenates the embeddings at each layer for each user and for each item such that $e_{u}^{(*)} = e_{u}^{(0)}||...||e_{u}^{(l)}$ and $e_{i}^{(*)} = e_{i}^{(0)}||...||e_{i}^{(l)}$ where $u$ indicates the user, $i$ indicates the item, $e$ is the embedding and $l$ is the layer.
Within the second component of message construction the message embedding is implemented as:
\begin{equation}
    m_{u \leftarrow i} = \frac{1}{\sqrt{|\mathcal{N}_u||\mathcal{N}_i|}}(W_1e_i + W_2(e_i \bigodot e_u)),
    \label{eq:message-construction}
\end{equation}
where $W_1$ and $W_2 \in R^{d' \times d}$ are weight matrices and $d'$ is the transformation size.
$\frac{1}{\sqrt{|\mathcal{N}_u||\mathcal{N}_i|}}$ is the graph Laplacian norm, where $\mathcal{N}_u$ and $\mathcal{N}_i$ are the set of neighbors of user $u$ and item $i$.
This is done with the purpose of calculating how much the item contributes to the users preference \cite{NGCF_2019}.
LightGCN criticize the use of the weight matrices $W_1$ and $W_2$ as not being useful for CF as each user-item interaction graph only has the ID as input and it has no semantic value \cite{lightgcn}.
Also within the second component the message aggregation is implemented as:
\begin{equation}
    e_{u}^{(l)} = \mbox{LeakyReLU}(m^{(l)}_{u \leftarrow u} + \sum^{}_{i \in \mathcal{N}} m^{(l)}_{u \leftarrow i}),
\end{equation}
where $e_{u}^{(l)}$ are the embeddings of user $u$ at layer $l$.
LeakyReLU is the activation function chosen in NGCF to allow encoding positive and small negative signals.
LightGCN shows that NGCF will perform better if the feature transformation is removed, and that the activation function has small effect when the feature transformation is included, but if feature transformation is disabled the activation function will have a negative impact on performance.
Light GCN also shows that NGCF will significantly improve if both the activation function and feature transformation are removed \cite{lightgcn}.

\subsection{Price-aware Recommendation with Graph Convolutional Networks}
Yu Zheng et al. develops an effective method called \textit{Price-aware User Preference-modeling (PUP)} that is used to create recommendations with focus on the price factor \cite{Priceaware}.
There are two main challenges.
The first is that the preferences of a user on item price are unknown, and can only be seen implicitly by previous purchases.
The second challenge is that the price of an item largely depends on what category the item is within.
The first problem they propose a model that creates a relationship between user-to-item and item-to-price using a Graph Convolutional Network, so that they can propagate the influence of price onto the users.
For the second problem they solve this by integrating the categories and items into the propagation process.
PUP represents the data as a heterogeneous undirected graph $G = (V,E)$ where the nodes in $V$ consist of user nodes $u \in U$, item nodes $i \in I$, category nodes $c \in \textbf{c}$, and price nodes $p \in \textbf{p}$.
The edges in $E$ consist of interaction edges $(u, i)$ with $R_{ui} = 1$ if there is an interaction between user $u$ and item $i$ and category edges $(i, \textbf{c}_i)$ and price edges $(i, \textbf{p}_i)$.
Traditional Latent Factor Models such as Matrix Factorization only take a single type of edge $(u, i)$ into account.
This makes them insufficient when multiple types such as $(u, p)$ and $(i, c)$ are introduced \cite{Priceaware}.
Hereby Yu Zheng et al. use a Graph Neural Network to learn the embeddings so that each node has a separate embedding $e' \in \mathbb{R}^d$ where d is the dimensions of the embeddings.
In GCN the nodes propagate its nearest neighbors, which could be user-item, item-price or item-category.
Embeddings from node j to node i are propagated as follows:
\begin{equation}
    t_{ji} = \frac{1}{|\mathcal{N}_i|}e'_j,
\end{equation}
where $\mathcal{N}_i$ is the set of neighbors for node $i$ and $e'_j$ is the embedding node j.
The updating rule is:
\begin{align*}
     & o_u = \sum_{j \in \{i \textrm{ with } R_{ui}=1 \} \cup \{ u\}}^{} t_{ju}                            \\
     & o_i = \sum_{j \in \{u \textrm{ with } R_{ui}=1 \} \cup \{ i, \textbf{c}_i \textbf{p}_i\}}^{} t_{ji} \\
     & o_c = \sum_{j \in \{i \textrm{ with } \textbf{c}_i=c \} \cup \{ c\}}^{} t_{jc}                      \\
     & o_p = \sum_{j \in \{i \textrm{ with } \textbf{p}_i=c \} \cup \{ p\}}^{} t_{jp}                      \\
     & e_f = \textrm{tanh}(o_f), f \in \{u, i, c, p\},
\end{align*}
where $e_u$, $e_i$, $e_c$, and $e_p$ are the embeddings for user $u$, item $i$, category $c$ and price $p$.
The intuition about this updating rule is that the node gets aggregated together with its neighbors and in each layer it will go one step further.
So for example a price nodes embedding will be aggregated together with all item node embeddings connected with this specific price.
By doing this items with the same price levels are expected to be more similar than items not in the same price level.
The intuition about the category is the same as with the price.
The final purchase prediction $s$ between user $u$ and item $i$ is formulated as follows:
\begin{equation}
    s = e^T_u e_i + e^T_u e_p + e^T_i e_p + \alpha (e^T_u e_c + e^T_u e_p + e^T_c e_p),
\end{equation}
where $\alpha$ is a hyper-parameter used to balance the categories influence on the prediction.
The results of a Top-K Recommendation Performance test showcase that PUP outperforms other state-of-the-art methods such as NGCF with an average improvement of 3.59 \% to 5.97 \% \cite{Priceaware}.
