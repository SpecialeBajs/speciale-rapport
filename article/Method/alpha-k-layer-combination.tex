\subsection{Optimizing layer combination}\label{fredsplit}
In this subsection we describe the methods that are mentioned in the following research question:
\begin{itemize}
    \item \textbf{RQ3}: How does adding ALC and BLC to LightGCN perform compared to other state of the art methods?
\end{itemize}
LightGCN uses weighted summation as their aggregation function when combining the different layers, where each layer has the same weight.
The hypothesis is that some layers are more important than others therefore giving each layer the same weight when doing summation can decrease the performance of the model.
Instead, we have developed two algorithms that take into account the performance of each layer to see if there is a substantial performance gain to be had when optimizing the layer combination.
Given a dataset, a GCN model will have a number of convolution layers.
The algorithms looks at the performance of each layer compared to the best performing layer.
Based on how much worse the layer performs compared to the best layer its weight when doing summation will be reduced and the other layers weight will be increased.

\subsubsection{Aggressive Layer Combination}
The pseudo-code for the ALC algorithm can be seen on \autoref{alg:aggresive-layereffect}.
Each layer starts out with the same weight.
Given an evaluation method score for each layer, the algorithm looks at how many percent worse each layer performed compared to the best layer.
The layers are then sorted with the worst performing layer first.
The first layer is then removed from the list and can not gain any weight, as seen when L is popped on \autoref{alg:aggresive-layereffect}.
For each layer, the amount of percent it has performed worse is subtracted from its weight and shared among the remaining layers to increase their weight.
This can leave multiple layers with having zero weight and therefore not be a part of the final embedding.
\\
This might not be optimal as even layers who might not perform well on their own can still provide some kind of collaborative signal.
This is seen in experiments where the 0th embedding was removed and yielded a worse result even though its individual result was poor.
Therefore a more balanced algorithm was thought to be better.

\begin{algorithm}
    \caption{Algorithm for the ALC based on performance}
    \SetAlgoLined
    \KwResult{A list of how much weight each layer has on the final embedding }
    L = Ordered list of performance of each layer \\
    weight  = List of current weight for each layer \\
    \While{i < L.length; i++}{
        weight [i] = 100 / L.length \\
    }
    \While{L.length > 1; i++}{
        worseP = L.pop()\\
        newWeight  = max(Inf[i] - worseP, 0)\\
        \While{k < L.length; k++}{
            weight[k] = weight[k] + ((weight[i] - newWeight ) / L.length)\\
        }
        weight[i] = newWeight \\
    }
    \Return{weight.orderBy(layerId)}
    \label{alg:aggresive-layereffect}
\end{algorithm}

\subsubsection{Balanced Layer Combination}
The pseudo-code for the BLC algorithm can be seen on \autoref{alg:bal-layereffect}.
The BLC algorithm works much the same as ALC but instead of popping a given layer of the list when its weight has been reduced, it is instead kept.
This means that when reducing other layers weight some of their weights will be given to the already reduced layer.
This insures no layer will ever reach zero weight as can happen in ALC.

\begin{algorithm}
    \caption{Algorithm for the BLC based on performance}
    \SetAlgoLined
    \KwResult{A list of how much weight each layer has on the final embedding }
    L = Ordered list of performance of each layer \\
    weight  = List of current weight for each layer \\
    \While{i < L.length; i++}{
        weight [i] = 100 / L.length \\
    }
    \While{i = 0; i < L.length; i++}{
        worseP = L.[i]\\
        newWeight  = max(weight[i] - worseP, 0)\\
        \While{k = 0; k < L.length; k++}{
            if(k != i){
                weight[k] = weight[k] + ((weight[i] - newWeight ) / (L.length-1))\\
                }
        }
        weight[i] = newWeight \\
    }
    \Return{weight.orderBy(layerId)}
    \label{alg:bal-layereffect}
\end{algorithm}
Examples of how the ALC and BLC works can be seen on \autoref{fig:fredsplitAgg} and \autoref{fig:fredsplitBal}
In both examples, we have 4 layers starting at layer 0 and ending at layer 3 and each layer has an initial weight of 25\%.
Their performance is color-coded to make it easier to follow which values are used to calculate the new weight.
The yellow squares mark which layer is getting a lower weight in the current iteration and green marks the specific layer's final weight.
Comparing these two algorithms it can be seen that there is a larger diversion in the aggressive algorithm than in the balanced algorithm.

\begin{figure}
    \includegraphics[width=0.5\textwidth]{figures/fredsplit/aggresiveAlgo.png}
    \centering
    \caption{Example of aggressive splitting of layer weight based on performance}
    \label{fig:fredsplitAgg}
\end{figure}

\begin{figure}
    \includegraphics[width=0.5\textwidth]{figures/fredsplit/balancedAlgo.png}
    \centering
    \caption{Example of balanced splitting of layer weight based on performance}
    \label{fig:fredsplitBal}
\end{figure}

