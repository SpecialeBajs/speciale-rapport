\subsection{The effect of $\alpha_k$ in LightGCN}\label{sec:method:alpha-k-effect}
Before trying to optimize $\alpha_k$ we wanted to investigate what impact $\alpha_k$ had on the performance.
This sections serves to answer the following research question:
\begin{itemize}
	\item \textbf{RQ1}: How does changing $\alpha_k$ affect the performance in LightGCN?
\end{itemize}
We experimented with three different methods to see how changes to how layer combination is performed could effect LightGCN's performance.
This includes removing $\alpha_k$, only using 1 layer and removing the 0th layer embedding.

\subsubsection{Removing $\alpha_k$}
The first method that we tried was removing $\alpha_k$.
If this did not have any substantial effect on the performance of LightGCN, it could indicate that it would not be worthwhile trying to optimize the layer combination method.
This was done by changing $\alpha_k$ to 1, which equates to summation of the different layer embeddings.
\begin{equation}
    \mathbf{e}_u = \sum_{k=0}^{K} \mathbf{e}_u^{(k)},
    \label{eq:removing-alpha-k-lightgcn-sum}
\end{equation}
This will make the embeddings scale when they are combined instead of normalizing them as they do in LightGCN.

\subsubsection{Utilizing only one layer}\label{subsubsec:only-1-layer}
The second method that we tried was only utilizing one layer.
This was done by performing the graph convolutions and then only using the embeddings from one of the layers instead of combining them.
This can be seen in \autoref{eq:only-utilizing-one-layer} where k is the layer that we choose to utilize. 
$\mathbf{e}^{(0)}$ will have zero convolutions, $\mathbf{e}^{(1)}$ will have one convolutions, $\mathbf{e}^{(2)}$ will have two convolutions and so on.
\begin{equation}
	\mathbf{e}_u = \mathbf{e}_u^{(k)}
	\label{eq:only-utilizing-one-layer}
\end{equation}

\subsubsection{Removing 0th layer}
The third method that we tried was removing 0th layer.
In this method we used the normal layer combination method used in LightGCN, but we did not include the 0th layer embedding in the final embedding as seen in \autoref{eq:removing-0th-layer}.
\begin{equation}
	\mathbf{e}_u = \sum_{k=1}^{K} \alpha_k \mathbf{e}_u^{(k)},
	\label{eq:removing-0th-layer}
\end{equation}
where $K$ is the number of layers and $\alpha_k$ is set to $1/K$.
